Preparing database...
Number of total splits in document (Super_Bowl_50) 1: 64
Number of total splits in document (Warsaw) 2: 85
Number of total splits in document (Normans) 3: 58
Number of total splits in document (Nikola_Tesla) 4: 85
Number of total splits in document (Computational_complexity_theory) 5: 60
Number of total splits in document (Teacher) 6: 64
Number of total splits in document (Martin_Luther) 7: 120
Number of total splits in document (Southern_California) 8: 35
Number of total splits in document (Sky_(United_Kingdom)) 9: 31
Number of total splits in document (Victoria_(Australia)) 10: 37
Number of total splits in document (Huguenot) 11: 71
Number of total splits in document (Steam_engine) 12: 65
Number of total splits in document (Oxygen) 13: 57
Number of total splits in document (1973_oil_crisis) 14: 38
Number of total splits in document (Apollo_program) 15: 79
Number of total splits in document (European_Union_law) 16: 114
Number of total splits in document (Amazon_rainforest) 17: 29
Number of total splits in document (Ctenophora) 18: 51
Number of total splits in document (Fresno,_California) 19: 45
Number of total splits in document (Packet_switching) 20: 40
Number of total splits in document (Black_Death) 21: 38
Number of total splits in document (Geology) 22: 37
Number of total splits in document (Newcastle_upon_Tyne) 23: 82
Number of total splits in document (Victoria_and_Albert_Museum) 24: 99
Number of total splits in document (American_Broadcasting_Company) 25: 127
Number of total splits in document (Genghis_Khan) 26: 84
Number of total splits in document (Pharmacy) 27: 36
Number of total splits in document (Immune_system) 28: 84
Number of total splits in document (Civil_disobedience) 29: 35
Number of total splits in document (Construction) 30: 33
Number of total splits in document (Private_school) 31: 39
Number of total splits in document (Harvard_University) 32: 41
Number of total splits in document (Jacksonville,_Florida) 33: 41
Number of total splits in document (Economic_inequality) 34: 62
Number of total splits in document (Doctor_Who) 35: 93
Number of total splits in document (University_of_Chicago) 36: 50
Number of total splits in document (Yuan_dynasty) 37: 97
Number of total splits in document (Kenya) 38: 81
Number of total splits in document (Intergovernmental_Panel_on_Climate_Change) 39: 33
Number of total splits in document (Chloroplast) 40: 97
Number of total splits in document (Prime_number) 41: 52
Number of total splits in document (Rhine) 42: 63
Number of total splits in document (Scottish_Parliament) 43: 57
Number of total splits in document (Islamism) 44: 43
Number of total splits in document (Imperialism) 45: 67
Number of total splits in document (United_Methodist_Church) 46: 77
Number of total splits in document (French_and_Indian_War) 47: 74
Number of total splits in document (Force) 48: 68
Total documents:  48
Total number of chunks:  3018
Avg number of chunks per document:  62.875
Vector DB saved at ./data/contextualRAG_SQuAD/vector_db_squad.pkl
Hybrid search is running.
Created index: contextual_bm25_index
Deleted Elasticsearch index: contextual_bm25_index

Example 1:
Question: In what year did Massachusetts first require children to be educated in schools?
Ground Truth: 1852
Prediction: 1852
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 2:
Question: When were stromules discovered?
Ground Truth: 1962
Prediction: There is no mention of stromules in the provided context
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 0
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.0000
	 BERTScore Precision: 0.7934
	 BERTScore Recall: 0.8755
	 BERTScore F1: 0.8325
	 Exact Match: 0
	 LLM as a judge: 0.0
--------------------------------------------------

Example 3:
Question: Which artist who had a major influence on the Gothic Revival is represented in the V&A's British galleries?
Ground Truth: Horace Walpole
Prediction: Horace Walpole
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 4:
Question: In 1890, who did the university decide to team up with?
Ground Truth: several regional colleges and universities
Prediction: The American Baptist Education Society and oil magnate John D. Rockefeller
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1250
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1250
	 BLEU: 0.1336
	 BERTScore Precision: 0.8122
	 BERTScore Recall: 0.8286
	 BERTScore F1: 0.8203
	 Exact Match: 0
	 LLM as a judge: 0.2
--------------------------------------------------

Example 5:
Question: Who got a touchdown making the score 10-7?
Ground Truth: Jonathan Stewart
Prediction: Jonathan Stewart
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 6:
Question: How many Examination Boards exist in India?
Ground Truth: 30
Prediction: 30 different Examination Boards or academic authorities conduct examinations for school leaving certificates
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1429
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1429
	 BLEU: 0.0132
	 BERTScore Precision: 0.7772
	 BERTScore Recall: 0.8277
	 BERTScore F1: 0.8016
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 7:
Question: Who started rumors in 2008 that ABC would sell its ten owned-and-operated stations?
Ground Truth: Caris & Co.
Prediction: There is no mention of anyone starting rumors in 2008 that ABC would sell its ten owned-and-operated stations
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 0
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.0261
	 BERTScore Precision: 0.8319
	 BERTScore Recall: 0.7915
	 BERTScore F1: 0.8112
	 Exact Match: 0
	 LLM as a judge: 0.0
--------------------------------------------------

Example 8:
Question: Which network broadcasted the 50th Super Bowl game? 
Ground Truth: CBS
Prediction: CBS and BBC
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.5000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.5000
	 BLEU: 0.2016
	 BERTScore Precision: 0.8281
	 BERTScore Recall: 0.9141
	 BERTScore F1: 0.8690
	 Exact Match: 0
	 LLM as a judge: 0.85
--------------------------------------------------

Example 9:
Question: Why was this short termed organization created?
Ground Truth: coordinate the response to the embargo
Prediction: to coordinate the response to the embargo
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.9231
	 ROUGE-2 F1: 0.9091
	 ROUGE-L F1: 0.9231
	 BLEU: 0.9254
	 BERTScore Precision: 0.9510
	 BERTScore Recall: 0.9701
	 BERTScore F1: 0.9605
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 10:
Question: What does LGM stands for?
Ground Truth: Last Glacial Maximum
Prediction: Little Green Men
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 0
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.1132
	 BERTScore Precision: 0.7882
	 BERTScore Recall: 0.8085
	 BERTScore F1: 0.7982
	 Exact Match: 0
	 LLM as a judge: 0.0
--------------------------------------------------

Example 11:
Question: What is the paper written by Richard Karp in 1972 that ushered in a new era of understanding between intractability and NP-complete problems?
Ground Truth: "Reducibility Among Combinatorial Problems"
Prediction: "Reducibility Among Combinatorial Problems"
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 12:
Question: What Lake in a German island Mainau receives a fraction of the Rhine's flow?
Ground Truth: Lake Überlingen
Prediction: Lake Überlingen
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 13:
Question: What studio does ABC own at 1500 Broadway in NYC?
Ground Truth: Times Square Studios
Prediction: The Times Square Studios
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8571
	 ROUGE-2 F1: 0.8000
	 ROUGE-L F1: 0.8571
	 BLEU: 0.8277
	 BERTScore Precision: 0.9025
	 BERTScore Recall: 0.9312
	 BERTScore F1: 0.9166
	 Exact Match: 0
	 LLM as a judge: 0.95
--------------------------------------------------

Example 14:
Question: How many fraternities are apart of the university?
Ground Truth: fifteen
Prediction: Fifteen
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 0.8366
	 BERTScore Precision: 0.8259
	 BERTScore Recall: 0.9287
	 BERTScore F1: 0.8743
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 15:
Question: At what address did Goldenson secure a new headquarters for ABC?
Ground Truth: 1330 Avenue of the Americas in Manhattan
Prediction: a 44 story building located at 1330 Avenue of the Americas in Manhattan, at the corner of 54th Street
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.5385
	 ROUGE-2 F1: 0.5000
	 ROUGE-L F1: 0.5385
	 BLEU: 0.3914
	 BERTScore Precision: 0.8674
	 BERTScore Recall: 0.9459
	 BERTScore F1: 0.9049
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 16:
Question: Who had established the Russian empire to its former glory prior to 1921?
Ground Truth: Bolshevik leaders
Prediction: Bolshevik leaders
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 17:
Question: What does the Riemann hypothesis state the source of irregularity in the distribution of points comes from?
Ground Truth: random noise
Prediction: random noise
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 18:
Question: Which work of Luther's is effective in teaching children?
Ground Truth: Small Catechism
Prediction: The Small Catechism proved especially effective in helping parents teach their children
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.2857
	 ROUGE-2 F1: 0.1667
	 ROUGE-L F1: 0.2857
	 BLEU: 0.1649
	 BERTScore Precision: 0.8493
	 BERTScore Recall: 0.9377
	 BERTScore F1: 0.8913
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 19:
Question: How was the civil disobedience shown in Antigone?
Ground Truth: obey her conscience rather than human law
Prediction: Antigone defied Creon by giving her brother Polynices a proper burial despite Creon's orders to the contrary and delivered a speech saying she must obey her conscience rather than human law
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.3590
	 ROUGE-2 F1: 0.3243
	 ROUGE-L F1: 0.3590
	 BLEU: 0.2138
	 BERTScore Precision: 0.8165
	 BERTScore Recall: 0.9186
	 BERTScore F1: 0.8646
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 20:
Question: Where do juvenile platyctenids live?
Ground Truth: among the plankton
Prediction: among the plankton
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 21:
Question: How often are elections held for the Victorian Parliament?
Ground Truth: every four years
Prediction: Elections for the Victorian Parliament are now fixed and occur in November every four years
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.3333
	 ROUGE-2 F1: 0.2500
	 ROUGE-L F1: 0.3333
	 BLEU: 0.1687
	 BERTScore Precision: 0.8041
	 BERTScore Recall: 0.8335
	 BERTScore F1: 0.8186
	 Exact Match: 0
	 LLM as a judge: 0.95
--------------------------------------------------

Example 22:
Question: What magnetic and electric force acts on a charge?
Ground Truth: unified electromagnetic
Prediction: The unified electromagnetic force acts on a charge as a sum of the electrostatic force and the magnetic force
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1905
	 ROUGE-2 F1: 0.1053
	 ROUGE-L F1: 0.1905
	 BLEU: 0.2054
	 BERTScore Precision: 0.8010
	 BERTScore Recall: 0.8934
	 BERTScore F1: 0.8447
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 23:
Question: What kinds of growth did Kublai encourage?
Ground Truth: commercial, scientific, and cultural
Prediction: Commercial, scientific, and cultural growth
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8889
	 ROUGE-2 F1: 0.8571
	 ROUGE-L F1: 0.8889
	 BLEU: 0.8106
	 BERTScore Precision: 0.9526
	 BERTScore Recall: 0.9695
	 BERTScore F1: 0.9610
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 24:
Question: What was the name of the first episode of the new Doctor Who series?
Ground Truth: Rose
Prediction: Kennedy
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 0
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.1615
	 BERTScore Precision: 0.9974
	 BERTScore Recall: 0.9974
	 BERTScore F1: 0.9974
	 Exact Match: 0
	 LLM as a judge: 0.2
--------------------------------------------------

Example 25:
Question: Each of the extended metropolitan areas has a population that exceeds what number?
Ground Truth: five million
Prediction: five million
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 26:
Question: Why is giving a defiant speech sometimes more harmful for the individual?
Ground Truth: lack of remorse
Prediction: It suggests a lack of remorse and an attempt to avoid responsibility for one's actions
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.3158
	 ROUGE-2 F1: 0.2353
	 ROUGE-L F1: 0.3158
	 BLEU: 0.1669
	 BERTScore Precision: 0.8508
	 BERTScore Recall: 0.9061
	 BERTScore F1: 0.8775
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 27:
Question: When were Joseph Schumpeter and Norman Angell at their most prolific writing period?
Ground Truth: before World War I
Prediction: Before World War I
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 0.9419
	 BERTScore Precision: 0.9992
	 BERTScore Recall: 0.9992
	 BERTScore F1: 0.9992
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 28:
Question: How do competing businesses attract workers?
Ground Truth: offering a higher wage
Prediction: By offering higher wages than their competitors
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.5455
	 ROUGE-2 F1: 0.2222
	 ROUGE-L F1: 0.5455
	 BLEU: 0.4080
	 BERTScore Precision: 0.8787
	 BERTScore Recall: 0.9204
	 BERTScore F1: 0.8991
	 Exact Match: 0
	 LLM as a judge: 0.95
--------------------------------------------------

Example 29:
Question: How many teams up to Super Bowl 50 have been to the championship game eight times?
Ground Truth: four
Prediction: Four teams have made eight appearances in the Super Bowl
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1818
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1818
	 BLEU: 0.0373
	 BERTScore Precision: 0.8114
	 BERTScore Recall: 0.8278
	 BERTScore F1: 0.8195
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 30:
Question: What plants create most electric power?
Ground Truth: steam turbine
Prediction: Steam turbines in various heat sources including nuclear reactors geothermal energy solar energy and waste heat from internal combustion engines or industrial processes
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1600
	 ROUGE-2 F1: 0.0870
	 ROUGE-L F1: 0.1600
	 BLEU: 0.0714
	 BERTScore Precision: 0.7745
	 BERTScore Recall: 0.9355
	 BERTScore F1: 0.8474
	 Exact Match: 0
	 LLM as a judge: 0.1
--------------------------------------------------

Example 31:
Question: The epidemic in Newcastle was the most what in any British city at the time?
Ground Truth: devastating loss
Prediction: devastating loss
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 32:
Question: What additional srevice did BSkyB offer besides Video on Demand that they claimed offered "substantially more value"?
Ground Truth: HD channels
Prediction: High Definition content
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.0900
	 BERTScore Precision: 0.8703
	 BERTScore Recall: 0.8872
	 BERTScore F1: 0.8787
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 33:
Question: Who did Tesla think would run the world of the future?
Ground Truth: women
Prediction: "Queen Bees" or women who would become the dominant sex in the future
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1429
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1429
	 BLEU: 0.0610
	 BERTScore Precision: 0.8036
	 BERTScore Recall: 0.8478
	 BERTScore F1: 0.8251
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 34:
Question: What have many HT members graduated to joining?
Ground Truth: terrorist groups
Prediction: terrorist groups
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 35:
Question: Approximately how many items are in the costume collection of the V&A?
Ground Truth: over 14,000
Prediction: Over 14,000 outfits plus accessories
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.6667
	 ROUGE-2 F1: 0.5714
	 ROUGE-L F1: 0.6667
	 BLEU: 0.2677
	 BERTScore Precision: 0.8710
	 BERTScore Recall: 0.9302
	 BERTScore F1: 0.8996
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 36:
Question: Which park is home to the Fresno Chafffee Zoo?
Ground Truth: Roeding Park
Prediction: Roeding Park
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 37:
Question: The point at which different radiometric isotopes stop diffusing into and out of the crystal lattice is called what?
Ground Truth: particular closure temperature
Prediction: closure temperature
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8000
	 ROUGE-2 F1: 0.6667
	 ROUGE-L F1: 0.8000
	 BLEU: 0.5605
	 BERTScore Precision: 0.9677
	 BERTScore Recall: 0.9082
	 BERTScore F1: 0.9370
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 38:
Question: What could someone be investigated for?
Ground Truth: allegations of professional misconduct
Prediction: Receiving a €60m bribe in connection with a Tobacco Products Directive
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 0
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.1257
	 BERTScore Precision: 0.8086
	 BERTScore Recall: 0.8547
	 BERTScore F1: 0.8310
	 Exact Match: 0
	 LLM as a judge: 0.2
--------------------------------------------------

Example 39:
Question: What are examples of economic actors?
Ground Truth: worker, capitalist/business owner, landlord
Prediction: Worker, capitalist/business owner, landlord
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 0.9763
	 BERTScore Precision: 0.9998
	 BERTScore Recall: 0.9998
	 BERTScore F1: 0.9998
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 40:
Question: What field involves the study of the immune system?
Ground Truth: Immunology
Prediction: Immunology
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 41:
Question: What are the phagocytes that are located in tissues in contact with the external environment called?
Ground Truth: Dendritic cells
Prediction: Dendritic cells
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 42:
Question: What did the merchants of Newcastle plot to do to Timothy Dexter?
Ground Truth: ruin him
Prediction: persuade him to sail a shipment of coal to Newcastle
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1667
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1667
	 BLEU: 0.0726
	 BERTScore Precision: 0.8341
	 BERTScore Recall: 0.9257
	 BERTScore F1: 0.8775
	 Exact Match: 0
	 LLM as a judge: 0.1
--------------------------------------------------

Example 43:
Question: What job did Tesla's father have in Gospic?
Ground Truth: pastor
Prediction: worked as a pastor
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.4000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.4000
	 BLEU: 0.3006
	 BERTScore Precision: 0.8140
	 BERTScore Recall: 0.8764
	 BERTScore F1: 0.8441
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 44:
Question: Which hotel did the Panthers stay at for the Super Bowl?
Ground Truth: San Jose Marriott.
Prediction: The San Jose Marriott
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8571
	 ROUGE-2 F1: 0.8000
	 ROUGE-L F1: 0.8571
	 BLEU: 0.8021
	 BERTScore Precision: 0.9261
	 BERTScore Recall: 0.9626
	 BERTScore F1: 0.9440
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 45:
Question: What are other alternative names for French and Indian War?
Ground Truth: Fourth Intercolonial War and the Great War for the Empire
Prediction: The Fourth Intercolonial War and the Great War for the Empire
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.9524
	 ROUGE-2 F1: 0.9474
	 ROUGE-L F1: 0.9524
	 BLEU: 0.9336
	 BERTScore Precision: 0.9736
	 BERTScore Recall: 0.9867
	 BERTScore F1: 0.9801
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 46:
Question: Outside of its use of automobiles, what else is southern California famous for using?
Ground Truth: highways
Prediction: petroleum, film, television, music, software, automotive, ports, finance, tourism, biomedical, and regional logistics
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.0153
	 BERTScore Precision: 0.7547
	 BERTScore Recall: 0.8606
	 BERTScore F1: 0.8041
	 Exact Match: 0
	 LLM as a judge: 0.05
--------------------------------------------------

Example 47:
Question: What was the last name of the designers of Newcastle's library?
Ground Truth: Green
Prediction: Green
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 48:
Question: How many people were on the test flight of the AS-206?
Ground Truth: unmanned
Prediction: There is no information about the number of people on the test flight of the AS-206 in the provided context
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.0208
	 BERTScore Precision: 0.8041
	 BERTScore Recall: 0.8522
	 BERTScore F1: 0.8275
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 49:
Question: What is the state average growth rate?
Ground Truth: 10.0%
Prediction: 10.0%
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 50:
Question: When was the study on sequenced Y genomes published?
Ground Truth: October 2011
Prediction: October 2011
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 51:
Question: How many times prios has the Sun Life Stadium had Super Bowls?
Ground Truth: 10.
Prediction: The text does not mention how many times the Sun Life Stadium has hosted Super Bowls, it only mentions that the South Florida/Miami area has hosted the event 10 times
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.0625
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0625
	 BLEU: 0.0086
	 BERTScore Precision: 0.8059
	 BERTScore Recall: 0.9122
	 BERTScore F1: 0.8558
	 Exact Match: 0
	 LLM as a judge: 0.1
--------------------------------------------------

Example 52:
Question: What does Kitab Rudjdjar mean in English?
Ground Truth: The Book of Roger
Prediction: The Book of Poisons
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 0
Generation:
	 ROUGE-1 F1: 0.7500
	 ROUGE-2 F1: 0.6667
	 ROUGE-L F1: 0.7500
	 BLEU: 0.6278
	 BERTScore Precision: 0.8928
	 BERTScore Recall: 0.9105
	 BERTScore F1: 0.9015
	 Exact Match: 0
	 LLM as a judge: 0.2
--------------------------------------------------

Example 53:
Question: Name one country that banned boating, driving and flying on Sundays.
Ground Truth: Germany
Prediction: The UK, Germany, Italy, Switzerland and Norway
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.2500
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.2500
	 BLEU: 0.1369
	 BERTScore Precision: 0.7642
	 BERTScore Recall: 0.8784
	 BERTScore F1: 0.8173
	 Exact Match: 0
	 LLM as a judge: 0.2
--------------------------------------------------

Example 54:
Question: For how long did the plague stick around?
Ground Truth: until the 19th century
Prediction: The plague recurred occasionally in Europe until the 19th century and was present in at least one location in the Islamic world virtually every year between 1500 and 1850
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.2424
	 ROUGE-2 F1: 0.1935
	 ROUGE-L F1: 0.2424
	 BLEU: 0.1255
	 BERTScore Precision: 0.8044
	 BERTScore Recall: 0.8689
	 BERTScore F1: 0.8354
	 Exact Match: 0
	 LLM as a judge: 0.95
--------------------------------------------------

Example 55:
Question: What channels were removed from the network in March of 2007?
Ground Truth: basic channels
Prediction: ABC Records and its 300 employees were laid off, the rights to the works of ABC Records and all of MCA's other labels have since been acquired by Universal Music Group
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 0
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.0329
	 BERTScore Precision: 0.7867
	 BERTScore Recall: 0.8328
	 BERTScore F1: 0.8091
	 Exact Match: 0
	 LLM as a judge: 0.0
--------------------------------------------------

Example 56:
Question: How many total touchdowns did Cam Newton score?
Ground Truth: 45
Prediction: Cam Newton scored 45 total touchdowns (35 passing, 10 rushing)
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1818
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1818
	 BLEU: 0.0231
	 BERTScore Precision: 0.7812
	 BERTScore Recall: 0.8256
	 BERTScore F1: 0.8027
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 57:
Question: How many original treaties establishing the EU protected fundamental rights?
Ground Truth: None
Prediction: None
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 58:
Question: What did BSkyB name their interactive service?
Ground Truth: Sky Active
Prediction: Open
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.0713
	 BERTScore Precision: 0.9310
	 BERTScore Recall: 0.8514
	 BERTScore F1: 0.8895
	 Exact Match: 0
	 LLM as a judge: 0.2
--------------------------------------------------

Example 59:
Question: How many inhabitants does the Los Angeles area contain?
Ground Truth: 12 million
Prediction: over 12 million inhabitants
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.6667
	 ROUGE-2 F1: 0.5000
	 ROUGE-L F1: 0.6667
	 BLEU: 0.3511
	 BERTScore Precision: 0.8690
	 BERTScore Recall: 0.8976
	 BERTScore F1: 0.8830
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 60:
Question: During what period did the Tower Theatre re-open?
Ground Truth: late 1970s
Prediction: the late 1970s
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8000
	 ROUGE-2 F1: 0.6667
	 ROUGE-L F1: 0.8000
	 BLEU: 0.6966
	 BERTScore Precision: 0.9405
	 BERTScore Recall: 0.9851
	 BERTScore F1: 0.9623
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 61:
Question: What did Warsz own?
Ground Truth: a village
Prediction: A village located at the modern-day site of Mariensztat neighbourhood
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.3077
	 ROUGE-2 F1: 0.1818
	 ROUGE-L F1: 0.3077
	 BLEU: 0.1087
	 BERTScore Precision: 0.7978
	 BERTScore Recall: 0.9379
	 BERTScore F1: 0.8622
	 Exact Match: 0
	 LLM as a judge: 0.6
--------------------------------------------------

Example 62:
Question: Who formed the government in Kenya?
Ground Truth: Kenya African National Union (KANU) of Jomo Kenyatta
Prediction: Jomo Kenyatta and the Kenya African National Union (KANU)
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8235
	 ROUGE-2 F1: 0.6667
	 ROUGE-L F1: 0.5882
	 BLEU: 0.8210
	 BERTScore Precision: 0.9302
	 BERTScore Recall: 0.9188
	 BERTScore F1: 0.9244
	 Exact Match: 0
	 LLM as a judge: 0.95
--------------------------------------------------

Example 63:
Question: What tool has measured the amount of dust that travels from the Sahara to the Amazon?
Ground Truth: NASA's CALIPSO satellite
Prediction: NASA's CALIPSO satellite
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 64:
Question: Who sacked Newton with 11 seconds left in the first half?
Ground Truth: DeMarcus Ware
Prediction: DeMarcus Ware
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 65:
Question: What was the hymn meant to examine students on?
Ground Truth: specific catechism questions
Prediction: specific catechism questions
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 66:
Question: In Nepalese private schools, what is the primary language of instruction?
Ground Truth: English
Prediction: English but Nepali and/or the state's official language is also taught as a compulsory subject
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1111
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1111
	 BLEU: 0.0664
	 BERTScore Precision: 0.7734
	 BERTScore Recall: 0.8805
	 BERTScore F1: 0.8235
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 67:
Question: What topic do private bills typically have?
Ground Truth: large-scale development projects
Prediction: large-scale development projects such as infrastructure projects that require the use of land or property
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.4000
	 ROUGE-2 F1: 0.3333
	 ROUGE-L F1: 0.4000
	 BLEU: 0.2997
	 BERTScore Precision: 0.8900
	 BERTScore Recall: 0.9753
	 BERTScore F1: 0.9307
	 Exact Match: 0
	 LLM as a judge: 0.95
--------------------------------------------------

Example 68:
Question: To where did Ethelred flee?
Ground Truth: Normandy
Prediction: Normandy
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 69:
Question: What future Revolutionary key figures participated in this attack?
Ground Truth: Washington and Thomas Gage
Prediction: George Washington and Thomas Gage
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8889
	 ROUGE-2 F1: 0.8571
	 ROUGE-L F1: 0.8889
	 BLEU: 0.7828
	 BERTScore Precision: 0.9064
	 BERTScore Recall: 0.9394
	 BERTScore F1: 0.9226
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 70:
Question: What style of lace is erroneously believed by some to have Huguenot influence?
Ground Truth: 'Bucks Point'
Prediction: Bucks Point
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 0.8338
	 BERTScore Precision: 0.9294
	 BERTScore Recall: 0.8563
	 BERTScore F1: 0.8914
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 71:
Question: What did the number of legions in Roman times depend on?
Ground Truth: threat of war
Prediction: whether a state or threat of war existed
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.5455
	 ROUGE-2 F1: 0.4444
	 ROUGE-L F1: 0.5455
	 BLEU: 0.3114
	 BERTScore Precision: 0.8253
	 BERTScore Recall: 0.8885
	 BERTScore F1: 0.8558
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 72:
Question: Before dinner what were Tesla's working hours?
Ground Truth: 9:00 a.m. until 6:00 p.m. or later
Prediction: from 9:00 a.m. military
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 0
Generation:
	 ROUGE-1 F1: 0.4706
	 ROUGE-2 F1: 0.4000
	 ROUGE-L F1: 0.4706
	 BLEU: 0.3088
	 BERTScore Precision: 0.9087
	 BERTScore Recall: 0.8741
	 BERTScore F1: 0.8911
	 Exact Match: 0
	 LLM as a judge: 0.2
--------------------------------------------------

Example 73:
Question: What house was the site of a weaving school in Canterbury?
Ground Truth: The Weavers
Prediction: The Weavers, a half-timbered house by the river
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.3636
	 ROUGE-2 F1: 0.2222
	 ROUGE-L F1: 0.3636
	 BLEU: 0.2209
	 BERTScore Precision: 0.8501
	 BERTScore Recall: 0.9190
	 BERTScore F1: 0.8832
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 74:
Question: What is strongly linked to good student-teacher relationships?
Ground Truth: student motivation and attitudes towards school
Prediction: Friendly and supportive teachers
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.2000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.2000
	 BLEU: 0.1589
	 BERTScore Precision: 0.8829
	 BERTScore Recall: 0.8731
	 BERTScore F1: 0.8780
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 75:
Question: What does the First Company Law Directive article 11 require?
Ground Truth: incorporations would only be nullified for a fixed list of reasons
Prediction: Incorporations would only be nullified for a fixed list of reasons
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 0.9847
	 BERTScore Precision: 0.9647
	 BERTScore Recall: 0.9766
	 BERTScore F1: 0.9706
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 76:
Question: What are two examples of cytotoxic or immunosuppressive drugs?
Ground Truth: methotrexate or azathioprine
Prediction: Methotrexate and azathioprine
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.6667
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.6667
	 BLEU: 0.8027
	 BERTScore Precision: 0.9579
	 BERTScore Recall: 0.9579
	 BERTScore F1: 0.9579
	 Exact Match: 0
	 LLM as a judge: 0.95
--------------------------------------------------

Example 77:
Question: What caused the need for principal Treaties that ended up forming the EU?
Ground Truth: common rules for coal and steel, and then atomic energy
Prediction: Common rules for coal and steel and atomic energy led to the establishment of more complete and formal institutions through the Treaty of Rome 1957 and the Maastricht Treaty 1992
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.4500
	 ROUGE-2 F1: 0.3684
	 ROUGE-L F1: 0.4500
	 BLEU: 0.2784
	 BERTScore Precision: 0.8551
	 BERTScore Recall: 0.9333
	 BERTScore F1: 0.8925
	 Exact Match: 0
	 LLM as a judge: 0.85
--------------------------------------------------

Example 78:
Question: Who originally led the Space Task Group?
Ground Truth: Robert R. Gilruth
Prediction: Robert R. Gilruth
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 79:
Question: Which female anchor from Today joined Hugh Downs on 20/20?
Ground Truth: Barbara Walters
Prediction: Barbara Walters
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 80:
Question: What is the estimate for the amount of tree species in the amazon tropical rain forest?
Ground Truth: 16,000 species
Prediction: The Amazon tropical rainforest is estimated to have 16,000 species of trees
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.3750
	 ROUGE-2 F1: 0.2857
	 ROUGE-L F1: 0.3750
	 BLEU: 0.1781
	 BERTScore Precision: 0.8475
	 BERTScore Recall: 0.9153
	 BERTScore F1: 0.8801
	 Exact Match: 0
	 LLM as a judge: 0.95
--------------------------------------------------

Example 81:
Question: What type of homes is Fresno known for?
Ground Truth: early twentieth century homes
Prediction: California Bungalow, American Craftsman, Spanish Colonial Revival, Mediterranean Revival, Mission Revival, and Storybook homes
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1111
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1111
	 BLEU: 0.0667
	 BERTScore Precision: 0.8022
	 BERTScore Recall: 0.8557
	 BERTScore F1: 0.8281
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 82:
Question: What team had the best start ever in the NFL?
Ground Truth: Carolina Panthers
Prediction: The Carolina Panthers started the season 14–0 setting franchise records for the best start and the longest single-season winning streak
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1667
	 ROUGE-2 F1: 0.0909
	 ROUGE-L F1: 0.1667
	 BLEU: 0.1209
	 BERTScore Precision: 0.8064
	 BERTScore Recall: 0.9338
	 BERTScore F1: 0.8655
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 83:
Question: Which organic compounds contain the greatest amount of oxygen by mass?
Ground Truth: carbohydrates
Prediction: Carbohydrates contain the largest proportion by mass of oxygen
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.2000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.2000
	 BLEU: 0.1869
	 BERTScore Precision: 0.7743
	 BERTScore Recall: 0.8153
	 BERTScore F1: 0.7943
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 84:
Question: What did Stiglitz present in 2009 regarding global inequality?
Ground Truth: evidence
Prediction: evidence that both global inequality and inequality within countries prevent growth by limiting aggregate demand
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1250
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1250
	 BLEU: 0.0647
	 BERTScore Precision: 0.7953
	 BERTScore Recall: 0.8654
	 BERTScore F1: 0.8289
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 85:
Question: Of what hollywood film company was UPT a subsidiary of when it merged with ABC?
Ground Truth: Paramount Pictures
Prediction: Paramount Pictures
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 86:
Question: When did the NFL announce that it would not use Roman numbers to designate the Super Bowl number?
Ground Truth: June 4, 2014
Prediction: On June 4, 2014
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8571
	 ROUGE-2 F1: 0.8000
	 ROUGE-L F1: 0.8571
	 BLEU: 0.7886
	 BERTScore Precision: 0.9096
	 BERTScore Recall: 0.9371
	 BERTScore F1: 0.9231
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 87:
Question: How many Huguenots were killed in Toulouse?
Ground Truth: Nearly 3,000
Prediction: Nearly 3,000
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 88:
Question: What is 'grey literature'?
Ground Truth: non-peer-reviewed sources
Prediction: Examples of non-peer-reviewed sources including model results, reports from government agencies and non-governmental organizations, and industry journals
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.3333
	 ROUGE-2 F1: 0.2727
	 ROUGE-L F1: 0.3333
	 BLEU: 0.1592
	 BERTScore Precision: 0.8398
	 BERTScore Recall: 0.9433
	 BERTScore F1: 0.8885
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 89:
Question: What's the name of Newcastle's speedway team?
Ground Truth: Newcastle Diamonds
Prediction: Newcastle Diamonds
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 90:
Question: What was the main reason for the show's suspension in 1989?
Ground Truth: Its late 1980s performance of three to five million viewers
Prediction: Its late 1980s performance of three to five million viewers was seen as poor at the time and was, according to the BBC Board of Control, a leading cause of the programme's 1989 suspension
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.4444
	 ROUGE-2 F1: 0.4186
	 ROUGE-L F1: 0.4444
	 BLEU: 0.3127
	 BERTScore Precision: 0.8705
	 BERTScore Recall: 0.9577
	 BERTScore F1: 0.9120
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 91:
Question: What is the name of the port city that was created?
Ground Truth: Mombasa
Prediction: Warsaw
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.0000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.0000
	 BLEU: 0.2558
	 BERTScore Precision: 0.8412
	 BERTScore Recall: 0.7335
	 BERTScore F1: 0.7837
	 Exact Match: 0
	 LLM as a judge: 0.0
--------------------------------------------------

Example 92:
Question: When did Martin Luther receive his Doctor of Theology?
Ground Truth: 19 October 1512
Prediction: On 19 October 1512
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8571
	 ROUGE-2 F1: 0.8000
	 ROUGE-L F1: 0.8571
	 BLEU: 0.8256
	 BERTScore Precision: 0.9616
	 BERTScore Recall: 0.9865
	 BERTScore F1: 0.9739
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 93:
Question: Which phylum is more complex than sponges?
Ground Truth: Ctenophores
Prediction: Cnidarians and ctenophores have three main cell layers and no intermediate jelly-like layer like more complex animals
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1053
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1053
	 BLEU: 0.0812
	 BERTScore Precision: 0.8021
	 BERTScore Recall: 0.9178
	 BERTScore F1: 0.8561
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 94:
Question: What other scientist influence Frank Burnet when he was formulating his theory of immunity?
Ground Truth: Niels Jerne
Prediction: Niels Jerne
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 1.0000
	 ROUGE-2 F1: 1.0000
	 ROUGE-L F1: 1.0000
	 BLEU: 1.0000
	 BERTScore Precision: 1.0000
	 BERTScore Recall: 1.0000
	 BERTScore F1: 1.0000
	 Exact Match: 1
	 LLM as a judge: 1.0
--------------------------------------------------

Example 95:
Question: How many volumes are contained in the library?
Ground Truth: 18 million volumes
Prediction: over 18 million volumes
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.8571
	 ROUGE-2 F1: 0.8000
	 ROUGE-L F1: 0.8571
	 BLEU: 0.7749
	 BERTScore Precision: 0.9520
	 BERTScore Recall: 0.9739
	 BERTScore F1: 0.9628
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 96:
Question: What does co-teaching get the students to focus on?
Ground Truth: learning
Prediction: Learning by providing a social networking support that allows them to reach their full cognitive potential
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.1176
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.1176
	 BLEU: 0.0608
	 BERTScore Precision: 0.7938
	 BERTScore Recall: 0.8564
	 BERTScore F1: 0.8239
	 Exact Match: 0
	 LLM as a judge: 0.5
--------------------------------------------------

Example 97:
Question: Of what mountain system are the Victorian Alps a part?
Ground Truth: Great Dividing Range
Prediction: The Great Dividing Range mountain system
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.6667
	 ROUGE-2 F1: 0.5714
	 ROUGE-L F1: 0.6667
	 BLEU: 0.4901
	 BERTScore Precision: 0.8352
	 BERTScore Recall: 0.8641
	 BERTScore F1: 0.8494
	 Exact Match: 0
	 LLM as a judge: 1.0
--------------------------------------------------

Example 98:
Question: What body part did Thomas Davis break during the NFC Championship Game?
Ground Truth: arm
Prediction: his right arm
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.5000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.5000
	 BLEU: 0.1685
	 BERTScore Precision: 0.8087
	 BERTScore Recall: 0.8945
	 BERTScore F1: 0.8494
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------

Example 99:
Question: Whose First Law of Motion says that unless acted upon be forces, objects would continue to move at a constant velocity?
Ground Truth: Newton
Prediction: Newton's
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.6667
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.6667
	 BLEU: 0.7194
	 BERTScore Precision: 0.9083
	 BERTScore Recall: 0.9318
	 BERTScore F1: 0.9199
	 Exact Match: 0
	 LLM as a judge: 0.8
--------------------------------------------------

Example 100:
Question: How was scarcity managed in many countries?
Ground Truth: rationing
Prediction: Rationing was used to address scarcity in many countries
Evaluation Scores:
Retrieval:
	 Precision@10 (Retrieval): 1
Generation:
	 ROUGE-1 F1: 0.2000
	 ROUGE-2 F1: 0.0000
	 ROUGE-L F1: 0.2000
	 BLEU: 0.1342
	 BERTScore Precision: 0.7915
	 BERTScore Recall: 0.9279
	 BERTScore F1: 0.8543
	 Exact Match: 0
	 LLM as a judge: 0.9
--------------------------------------------------


Average Results:
Total Queries: 100
Number of retrieved chunks after hybrid search and reciprocal rank fusion: top-15
Number of retrieved chunks after reranking: top-10
Retrieval:
	Precision@10: 0.9200
	Percentage of results from semantic search in the top-15 before reranking: 68.47%
	Percentage of results from BM25 in the top-15 before reranking: 31.53%
Generation: 
	ROUGE-1 F1: 0.5810
	ROUGE-2 F1: 0.4398
	ROUGE-L F1: 0.5786
	BERTScore Precision: 0.8973
	BERTScore Recall: 0.9306
	BERTScore F1: 0.9129
	BLEU: 0.5249
	Exact Matching:  0.28
	LLM as a judge: 0.7915


END

--------------------------------------------------
--------------------------------------------------
